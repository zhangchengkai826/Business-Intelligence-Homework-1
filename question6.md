# Question 6

## Aprioi Algorithm

### Description

Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis. [1]

Apriori is an algorithm for mining frequent itemsets for Boolean association rules. It uses the Apriori property: all nonempty subsets of a frequent itemset must also be frequent, since A∪B pattern can not occur more frequently than A. Apriori property belongs to a special category of properties called antimonotonicity in the sense that if a set cannot pass a test, all of its supersets will fail the same test as well. Apriori property is used to reduce the search space to improve the efficiency of the level‐wise generation of frequent itemsets.

Apriori algorithm uses prior knowledge of frequent itemset properties and employs an iterative approach known as a level‐wise search, where k‐itemsets are used to explore (k+1)‐itemsets. First, the set of frequent 1‐itemsets is found and denoted by L1; then use L1 to find L2, the set of frequent 2‐itemsets, which is used to find L3, and so on, until no more frequent k‐itemsets can be found. The finding of each Lk requires one full scan of the database.

Apriori algorithm consists of **join** and **prune** actions. The join step: To find Lk, a set of candidate k‐itemsets is generated by joining with Lk‐1 itself. This set of candidates is denoted Ck. The two elements L1 and L2 in Lk‐1 are joinable if: l1[1]=l2[1]) & (l1[2]=l2[2]) & ... & (l1[k-2]=l2[k-2]) & (l1[k-1] less than l2[k-1]).

Ck is the superset of Lk, that is, its members may or may not be frequent, but all of the frequent k‐itemsets are included in Ck. Thus a database scan to determine the count of each candidate in Ck would result in the determination of Lk. To reduce the size of Ck, the Apriori property is used, that is, if any (k‐1)‐subset of a candidate k‐itemset is not in Lk‐1, then the candidate cannot be frequent either and so can be removed from Ck.

### Pseudocode

```
let minimum_support_count = <user_input>;
let minimum_confidence = <user_input>;
let C[] = { <all_1-itemsets> };
let frequent_itemsets[] = {};

do {
    scan database, calculate support_count of each itemset in C;
    foreach(itemset in C) {
        if(itemset.support_count < minimum_support_count) {
            remove itemset from C;
        }
    }
    frequent_itemsets += C;
    
    C = C join C;
    use Apriori property to prune C;
} while(C is not empty);

foreach(itemset in frequent_itemsets) {
    foreach(s in non-empty_subsets(itemset)) {
        if(itemset.support_count / s.support_count >= minimum_confidence) {
            output the rule: s => (itemset - s);
        }
    }
}
```

### Implementation

```

```

### Test

### Reference

[1] Aprioi Algorithm - Wikipedia (https://en.wikipedia.org/wiki/Apriori_algorithm)

## FP-Growth Algorithm

### Description

Frequent‐pattern growth adopts a divide‐andconquer strategy as follows: First, it compresses the database representing frequent items into a frequent pattern tree, FP‐tree. It then divides the compressed database into as set of conditional databases, each associated with one frequent item or “pattern fragment”, and mines each database separately.

### Pseudocode

```
struct TreeNode {
    var parent;
    var[] children;
    var item;
    var count;
    var next;
}

let minimum_support_count = <user_input>;
let minimum_confidence = <user_input>;
let all_1-itemsets[] = { <all_1-itemsets> };
let frequent_itemsets[] = {};
let root = new TreeNode();

scan database, calculate support_count of each itemset in all_1-itemsets;
sort itemsets in all_1-itemsets in descending order by support_count;

foreach(record in database) {
    sort item in record in the same order as all_1-itemsets;
    let node = root;
    foreach(item in record) {
        if(node.children does not contain a child which child.item == item) {
            let new_node = new TreeNode(
                child.item = item, child.count = 1, child.parent = node
                , child.next = all_1-itemsets.find(itemset[0] = item)[0].next
            );
            node.children.append(new_node);
            all_1-itemsets.find(itemset[0] = item)[0].next = new_node;
        } else {
            node.children.find(child.item = item).item++;
        }
        node = node.children.find(child.item = item);
    }
}

sort itemsets in all_1-itemsets in ascending order by support_count;
foreach(1-itemset in all_1-itemsets) {
    if(1-itemset.support_count >= minimum_support_count) {
        frequent_itemsets.append(1-itemset);
    }

    let item = 1-itemset[0];
    let node = item.next;
    let conditional_pattern_base = {};
    while(node.next != null) {
        let p = node.parent;
        let cnt = node.count;
        let cpb_obj = new Object();
        cpb_obj.cnt = p.item;
        while(p.parent != null) {
            if(p.parent.parent == null) {
                cpb_obj.subtreeId = p.item;
            }
            cpb_obj.items.append(p.item);
            p = p.parent;
        }
        conditional_pattern_base.append(cpb_obj);
        node = node.next;
    }

    let cond_fp_tree = new Dictionary<var, Dictionary<var, var>>();
    foreach(cpb in conditional_pattern_base) {
        foreach(item in cpb.items) {
            cond_fp_tree[cpb.subtreeId][item]++;
        }
    }
    foreach((subtreeId, dic) in cond_fp_tree) {
        foreach((item, count) in dic) {
            if(count < minimum_support_count) {
                dic.erase(item);
            }
        }
        if(dic.size() == 0) {
            cond_fp_tree.erase(dic);
        }
    }

    let fp_gen = new Dictionary();
    foreach((subtreeId, dic) in cond_fp_tree) {
        foreach(s in proper_non-empty_subset(dic.keys())) {
            let min_cnt = INF;
            foreach(item in s) {
                if(dic[item] < min_cnt) {
                    min_cnt = dic[item];
                }
            }
            s.insert(1-itemset[0]);
            fp_gen[s] += min_cnt;
        }
    }
    foreach(s in fp_gen.keys()) {
        s.support_count = fp_gen[s]
        frequent_itemsets.append(s);
    }
}

foreach(itemset in frequent_itemsets) {
    foreach(s in non-empty_subsets(itemset)) {
        if(itemset.support_count / s.support_count >= minimum_confidence) {
            output the rule: s => (itemset - s);
        }
    }
}
```

### Implementation

```
```

### Test
